{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvf3QhxhdCaj"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we will use KerasHub to build a scaled down Generative\n",
    "Pre-Trained (GPT) model. GPT is a Transformer-based model that allows you to generate\n",
    "sophisticated text from a prompt.\n",
    "\n",
    "We will train the model on the [simplebooks-92](https://arxiv.org/abs/1911.12391) corpus,\n",
    "which is a dataset made from several novels. It is a good dataset for this example since\n",
    "it has a small vocabulary and high word frequency, which is beneficial when training a\n",
    "model with few parameters.\n",
    "\n",
    "This example combines concepts from\n",
    "[Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)\n",
    "with KerasHub abstractions. We will demonstrate how KerasHub tokenization, layers and\n",
    "metrics simplify the training\n",
    "process, and then show how to generate output text using the KerasHub sampling utilities.\n",
    "\n",
    "Note: If you are running this example on a Colab,\n",
    "make sure to enable GPU runtime for faster training.\n",
    "\n",
    "This example requires KerasHub. You can install it via the following command:\n",
    "`pip install keras-hub`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYguvqvVdCak"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.0 in c:\\users\\zlism\\anaconda3\\lib\\site-packages (1.26.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow[and-cuda]==2.18\n",
      "  Obtaining dependency information for tensorflow[and-cuda]==2.18 from https://files.pythonhosted.org/packages/84/76/c55967ac9968ddaede25a4dce37aba37e9030656f02c12676151ce1b6f22/tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (3.4.0)\n",
      "Requirement already satisfied: packaging in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (68.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (1.67.1)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow[and-cuda]==2.18)\n",
      "  Obtaining dependency information for tensorboard<2.19,>=2.18 from https://files.pythonhosted.org/packages/b1/de/021c1d407befb505791764ad2cbd56ceaaa53a746baed01d2e2143f05f18/tensorboard-2.18.0-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow[and-cuda]==2.18)\n",
      "  Obtaining dependency information for keras>=3.5.0 from https://files.pythonhosted.org/packages/c2/88/eef50051a772dcb4433d1f3e4c1d6576ba450fe83e89d028d7e8b85a2122/keras-3.6.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (1.26.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (3.12.1)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow[and-cuda]==2.18)\n",
      "  Obtaining dependency information for ml-dtypes<0.5.0,>=0.4.0 from https://files.pythonhosted.org/packages/28/bc/6a2344338ea7b61cd7b46fb24ec459360a5a0903b57c55b156c1e46c644a/ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (0.37.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.5.3.2 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (12.5.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.5.82 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.5.82 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.5.82 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.5.82 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.3.0.75 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (9.3.0.75)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.3.61 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (11.2.3.61)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.6.82 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (10.3.6.82)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.3.83 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (11.6.3.83)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.1.3 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (12.5.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.5.82 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow[and-cuda]==2.18) (12.5.82)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]==2.18) (0.41.2)\n",
      "Requirement already satisfied: rich in /home/aec4hr/.local/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow[and-cuda]==2.18) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/aec4hr/.local/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow[and-cuda]==2.18) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/aec4hr/.local/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow[and-cuda]==2.18) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]==2.18) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]==2.18) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]==2.18) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]==2.18) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]==2.18) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]==2.18) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]==2.18) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow[and-cuda]==2.18) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]==2.18) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]==2.18) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aec4hr/.local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow[and-cuda]==2.18) (0.1.2)\n",
      "Using cached keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "Installing collected packages: ml-dtypes, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.2.0\n",
      "    Uninstalling ml-dtypes-0.2.0:\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.2\n",
      "    Uninstalling tensorboard-2.15.2:\n",
      "      Successfully uninstalled tensorboard-2.15.2\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/aec4hr/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.15.0\n",
      "    Uninstalling tensorflow-2.15.0:\n",
      "      Successfully uninstalled tensorflow-2.15.0\n",
      "\u001b[33m  WARNING: The scripts import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/aec4hr/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed keras-3.6.0 ml-dtypes-0.4.1 tensorboard-2.18.0 tensorflow-2.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow[and-cuda]==2.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 20:28:36.499503: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-09 20:28:36.612030: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731202116.645054  491318 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731202116.656536  491318 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-09 20:28:36.722996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "TensorFlow Version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras-nlp in /home/aec4hr/.local/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: keras==3.6.0 in /home/aec4hr/.local/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: keras-hub==0.17.0 in /home/aec4hr/.local/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: absl-py in /home/aec4hr/.local/lib/python3.11/site-packages (from keras==3.6.0) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/aec4hr/.local/lib/python3.11/site-packages (from keras==3.6.0) (1.26.0)\n",
      "Requirement already satisfied: rich in /home/aec4hr/.local/lib/python3.11/site-packages (from keras==3.6.0) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/aec4hr/.local/lib/python3.11/site-packages (from keras==3.6.0) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/aec4hr/.local/lib/python3.11/site-packages (from keras==3.6.0) (3.12.1)\n",
      "Requirement already satisfied: optree in /home/aec4hr/.local/lib/python3.11/site-packages (from keras==3.6.0) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in /home/aec4hr/.local/lib/python3.11/site-packages (from keras==3.6.0) (0.4.1)\n",
      "Requirement already satisfied: packaging in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from keras==3.6.0) (23.1)\n",
      "Requirement already satisfied: regex in /home/aec4hr/.local/lib/python3.11/site-packages (from keras-hub==0.17.0) (2024.11.6)\n",
      "Requirement already satisfied: kagglehub in /home/aec4hr/.local/lib/python3.11/site-packages (from keras-hub==0.17.0) (0.3.3)\n",
      "Requirement already satisfied: tensorflow-text in /home/aec4hr/.local/lib/python3.11/site-packages (from keras-hub==0.17.0) (2.18.0)\n",
      "Requirement already satisfied: requests in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from kagglehub->keras-hub==0.17.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from kagglehub->keras-hub==0.17.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from optree->keras==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from rich->keras==3.6.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from rich->keras==3.6.0) (2.16.1)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow-text->keras-hub==0.17.0) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aec4hr/.local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras==3.6.0) (0.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (4.25.5)\n",
      "Requirement already satisfied: setuptools in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (68.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from requests->kagglehub->keras-hub==0.17.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from requests->kagglehub->keras-hub==0.17.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from requests->kagglehub->keras-hub==0.17.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from requests->kagglehub->keras-hub==0.17.0) (2023.7.22)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (0.41.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/aec4hr/.local/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade keras-nlp keras==3.6.0 keras-hub==0.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 3.6.0\n",
      "KerasNLP version: 0.17.0\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "import keras\n",
    "\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"KerasNLP version: {keras_nlp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LYOf-udhdCal",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import keras_hub\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvijwNNUdCal"
   },
   "source": [
    "## Settings & hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1QMQ69O1dCal",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 64\n",
    "MIN_STRING_LEN = 512  # Strings shorter than this will be discarded\n",
    "SEQ_LEN = 128  # Length of training sequences, in tokens\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 128\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 5000  # Limits parameters in model.\n",
    "\n",
    "# Training\n",
    "EPOCHS = 5\n",
    "\n",
    "# Inference\n",
    "NUM_TOKENS_TO_GENERATE = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfoHrqxRdCam"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "Now, let's download the dataset! The SimpleBooks dataset consists of 1,573 Gutenberg books, and has\n",
    "one of the smallest vocabulary size to word-level tokens ratio. It has a vocabulary size of ~98k,\n",
    "a third of WikiText-103's, with around the same number of tokens (~100M). This makes it easy to fit a small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "S6AIfJrBdCam"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'keras.utils.get_file(\\n    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\\n    extract=True,\\n)\\n#dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\") this is the original line of code\\ndir = os.path.join(os.path.expanduser(\"~/.keras/datasets\"), \"simplebooks\", \"simplebooks-92-raw\")\\n\\n# Load simplebooks-92 train set and filter out short lines.\\nraw_train_ds = (\\n    tf_data.TextLineDataset(dir + \"train.txt\")\\n    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\\n    .batch(BATCH_SIZE)\\n    .shuffle(buffer_size=256)\\n)\\n\\n# Load simplebooks-92 validation set and filter out short lines.\\nraw_val_ds = (\\n    tf_data.TextLineDataset(dir + \"valid.txt\")\\n    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\\n    .batch(BATCH_SIZE)\\n)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"keras.utils.get_file(\n",
    "    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "#dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\") this is the original line of code\n",
    "dir = os.path.join(os.path.expanduser(\"~/.keras/datasets\"), \"simplebooks\", \"simplebooks-92-raw\")\n",
    "\n",
    "# Load simplebooks-92 train set and filter out short lines.\n",
    "raw_train_ds = (\n",
    "    tf_data.TextLineDataset(dir + \"train.txt\")\n",
    "    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(buffer_size=256)\n",
    ")\n",
    "\n",
    "# Load simplebooks-92 validation set and filter out short lines.\n",
    "raw_val_ds = (\n",
    "    tf_data.TextLineDataset(dir + \"valid.txt\")\n",
    "    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hEszN9BjQ6F",
    "outputId": "308ff983-67ef-477f-e9cc-f7ec865a1c45",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to: /sfs/gpfs/tardis/home/aec4hr/Codeathon3/simplebooks/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731202144.465121  491318 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43483 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:63:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "#Get current working directory\n",
    "cwd = os.getcwd()\n",
    "#Download the dataset to the current working directory\n",
    "file_path = keras.utils.get_file(\n",
    "   fname=\"simplebooks.zip\",\n",
    "   origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\"\n",
    ",\n",
    "   extract=False,  # Do not extract immediately\n",
    "   cache_dir=cwd  # Save it in the current working directory\n",
    ")\n",
    "#Extract the zip file manually to the current working directory\n",
    "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "   zip_ref.extractall(cwd)\n",
    "#Now set the dataset directory based on your current working directory\n",
    "dir = os.path.join(cwd, \"simplebooks/\")\n",
    "#Load simplebooks-92 train set and filter out short lines.\n",
    "raw_train_ds = (\n",
    "   tf_data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
    "   .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n",
    "   .batch(BATCH_SIZE)\n",
    "   .shuffle(buffer_size=256)\n",
    ")\n",
    "#Load simplebooks-92 validation set and filter out short lines.\n",
    "raw_val_ds = (\n",
    "   tf_data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n",
    "   .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n",
    "   .batch(BATCH_SIZE)\n",
    ")\n",
    "print(f\"Dataset extracted to: {dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aJpGFjHdCan"
   },
   "source": [
    "## Train the tokenizer\n",
    "\n",
    "We train the tokenizer from the training dataset for a vocabulary size of `VOCAB_SIZE`,\n",
    "which is a tuned hyperparameter. We want to limit the vocabulary as much as possible, as\n",
    "we will see later on\n",
    "that it has a large effect on the number of model parameters. We also don't want to include\n",
    "*too few* vocabulary terms, or there would be too many out-of-vocabulary (OOV) sub-words. In\n",
    "addition, three tokens are reserved in the vocabulary:\n",
    "\n",
    "- `\"[PAD]\"` for padding sequences to `SEQ_LEN`. This token has index 0 in both\n",
    "`reserved_tokens` and `vocab`, since `WordPieceTokenizer` (and other layers) consider\n",
    "`0`/`vocab[0]` as the default padding.\n",
    "- `\"[UNK]\"` for OOV sub-words, which should match the default `oov_token=\"[UNK]\"` in\n",
    "`WordPieceTokenizer`.\n",
    "- `\"[BOS]\"` stands for beginning of sentence, but here technically it is a token\n",
    "representing the beginning of each line of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "u2LpE-5ldCan",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 20:29:54.374525: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Train tokenizer vocabulary\n",
    "vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
    "    raw_train_ds,\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    lowercase=True,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myqO_vnwdCan"
   },
   "source": [
    "## Load tokenizer\n",
    "\n",
    "We use the vocabulary data to initialize\n",
    "`keras_hub.tokenizers.WordPieceTokenizer`. WordPieceTokenizer is an efficient\n",
    "implementation of the WordPiece algorithm used by BERT and other models. It will strip,\n",
    "lower-case and do other irreversible preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NJfvuumEdCao",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkjBACbWdCao"
   },
   "source": [
    "## Tokenize data\n",
    "\n",
    "We preprocess the dataset by tokenizing and splitting it into `features` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bOth0uJ1dCao",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# packer adds a start token\n",
    "start_packer = keras_hub.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs):\n",
    "    outputs = tokenizer(inputs)\n",
    "    features = start_packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Tokenize and split into train and label sequences.\n",
    "train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf_data.AUTOTUNE).prefetch(\n",
    "    tf_data.AUTOTUNE\n",
    ")\n",
    "val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf_data.AUTOTUNE).prefetch(\n",
    "    tf_data.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjvjJTjVdCao"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "We create our scaled down GPT model with the following layers:\n",
    "\n",
    "- One `keras_hub.layers.TokenAndPositionEmbedding` layer, which combines the embedding\n",
    "for the token and its position.\n",
    "- Multiple `keras_hub.layers.TransformerDecoder` layers, with the default causal masking.\n",
    "The layer has no cross-attention when run with decoder sequence only.\n",
    "- One final dense linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ereq3BNadCao",
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embedding.\n",
    "embedding_layer = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "# Transformer decoders.\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_hub.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "    )\n",
    "    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
    "# Output.\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_hub.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8g8jb08ZdCao"
   },
   "source": [
    "Let's take a look at our model summary - a large majority of the\n",
    "parameters are in the `token_and_position_embedding` and the output `dense` layer!\n",
    "This means that the vocabulary size (`VOCAB_SIZE`) has a large effect on the size of the model,\n",
    "while the number of Transformer decoder layers (`NUM_LAYERS`) doesn't affect it as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "wW2VGo8kdCao",
    "outputId": "d314cd1e-4301-40fe-ed82-479c2e3ec098",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">329,085</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_decoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">329,085</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m1,312,768\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m329,085\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_decoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m329,085\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m)     │     \u001b[38;5;34m1,285,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,255,938</span> (12.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,255,938\u001b[0m (12.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,255,938</span> (12.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,255,938\u001b[0m (12.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TukIktfrdCap"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our model, let's train it with the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyrLtThbdCap",
    "outputId": "3752b927-6916-415a-a9cf-a55aba0c61c7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 09:35:51.799070: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "/home/aec4hr/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'position_embedding' (of type PositionEmbedding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/aec4hr/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/aec4hr/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/aec4hr/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731162959.363130  285871 service.cc:148] XLA service 0x7f9c88008cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731162959.363198  285871 service.cc:156]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6\n",
      "2024-11-09 09:35:59.448800: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1731162959.593244  285871 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1731162959.752901  285871 assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1731162959.803622  285871 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-09 09:35:59.899494: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "2024-11-09 09:36:00.378204: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53_0', 136 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:00.443500: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:00.580980: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:01.028540: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 452 bytes spill stores, 464 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:01.077365: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_60', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:01.118922: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_60', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:01.160136: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:01.287377: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_59', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:01.433596: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18', 528 bytes spill stores, 528 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:01.931146: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18', 240 bytes spill stores, 240 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:01.993717: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18', 148 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:02.036083: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18_0', 564 bytes spill stores, 564 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:02.755703: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3441_0', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:02.792127: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 868 bytes spill stores, 844 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16/Unknown \u001b[1m14s\u001b[0m 10ms/step - loss: 7.7993 - perplexity: 2610.7571"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731162965.550960  285871 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2221/Unknown \u001b[1m54s\u001b[0m 18ms/step - loss: 5.0716 - perplexity: 195.1689"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1731163005.508260  285871 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1731163005.619780  285871 assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2024-11-09 09:36:46.495223: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:46.856824: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18_0', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2442/Unknown \u001b[1m60s\u001b[0m 19ms/step - loss: 5.0322 - perplexity: 186.8458"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 09:36:51.924627: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "W0000 00:00:1731163012.993253  285867 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1731163012.995957  285867 assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2024-11-09 09:36:53.571944: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-09 09:36:53.820334: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_19', 452 bytes spill stores, 464 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 20ms/step - loss: 5.0316 - perplexity: 186.7414 - val_loss: 4.2358 - val_perplexity: 69.2680\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 09:36:54.606595: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-09 09:36:54.606661: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4327584292250278623\n",
      "2024-11-09 09:36:54.606683: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6965722924101409755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 17ms/step - loss: 4.1923 - perplexity: 66.2508 - val_loss: 4.1133 - val_perplexity: 61.2603\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 09:37:40.960132: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-09 09:37:40.960224: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4327584292250278623\n",
      "2024-11-09 09:37:40.960241: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6965722924101409755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 18ms/step - loss: 4.0402 - perplexity: 56.8730 - val_loss: 4.0260 - val_perplexity: 56.1025\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 09:38:30.166531: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4327584292250278623\n",
      "2024-11-09 09:38:30.166611: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6965722924101409755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2441/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.9668 - perplexity: 52.8465"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 09:39:22.737333: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4327584292250278623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 19ms/step - loss: 3.9668 - perplexity: 52.8449 - val_loss: 3.9926 - val_perplexity: 54.2658\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 09:39:23.052006: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-09 09:39:23.052084: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4327584292250278623\n",
      "2024-11-09 09:39:23.052100: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6965722924101409755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 20ms/step - loss: 3.9180 - perplexity: 50.3251 - val_loss: 3.9583 - val_perplexity: 52.4085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 09:40:17.326623: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4327584292250278623\n",
      "2024-11-09 09:40:17.326704: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6965722924101409755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f9d14382710>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjVOPvC8dCap"
   },
   "source": [
    "## Inference\n",
    "\n",
    "With our trained model, we can test it out to gauge its performance. To do this\n",
    "we can seed our model with an input sequence starting with the `\"[BOS]\"` token,\n",
    "and progressively sample the model by making predictions for each subsequent\n",
    "token in a loop.\n",
    "\n",
    "To start lets build a prompt with the same shape as our model inputs, containing\n",
    "only the `\"[BOS]\"` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqY2CoMldCap",
    "outputId": "9152df26-26fc-4967-fd42-bd53811bf230",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The \"packer\" layers adds the [BOS] token for us.\n",
    "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AQYMjGgdCaq"
   },
   "source": [
    "We will use the `keras_hub.samplers` module for inference, which requires a\n",
    "callback function wrapping the model we just trained. This wrapper calls\n",
    "the model and returns the logit predictions for the current token we are\n",
    "generating.\n",
    "\n",
    "Note: There are two pieces of more advanced functionality available when\n",
    "defining your callback. The first is the ability to take in a `cache` of states\n",
    "computed in previous generation steps, which can be used to speed up generation.\n",
    "The second is the ability to output the final dense \"hidden state\" of each\n",
    "generated token. This is used by `keras_hub.samplers.ContrastiveSampler`, which\n",
    "avoids repetition by penalizing repeated hidden states. Both are optional, and\n",
    "we will ignore them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4kWt0wczdCaq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def next(prompt, cache, index):\n",
    "    logits = model(prompt)[:, index - 1, :]\n",
    "    # Ignore hidden states for now; only needed for contrastive search.\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkC5efVDdCaq"
   },
   "source": [
    "Creating the wrapper function is the most complex part of using these functions. Now that\n",
    "it's done, let's test out the different utilities, starting with greedy search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewuaZOjedCaq"
   },
   "source": [
    "### Greedy search\n",
    "\n",
    "We greedily pick the most probable token at each timestep. In other words, we get the\n",
    "argmax of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTmMEMJTdCaq",
    "outputId": "19c48894-f5f5-4ac6-9df3-e3fd08c6f19e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy search generated text: \n",
      "['[BOS] \" i have been thinking of the matter , \" the captain said , \" but i have been thinking of the matter over the matter of the saxons , and i have been able to tell you that the captain of the captain of the captain , who had been a shipwright , and had been in command of the ship , and had been a shipwright crew of the ship , and had been sent ashore , and had been sent ashore to the ship , and had been sent ashore to the ship , and had been sent ashore to the ship , and had been drowned in the ship , and had been drowned in the ship']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.GreedySampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,  # Start sampling immediately after the [BOS] token.\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4qkG4LvdCat"
   },
   "source": [
    "As you can see, greedy search starts out making some sense, but quickly starts repeating\n",
    "itself. This is a common problem with text generation that can be fixed by some of the\n",
    "probabilistic text generation utilities shown later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OEr7qLldCat"
   },
   "source": [
    "### Beam search\n",
    "\n",
    "At a high-level, beam search keeps track of the `num_beams` most probable sequences at\n",
    "each timestep, and predicts the best next token from all sequences. It is an improvement\n",
    "over greedy search since it stores more possibilities. However, it is less efficient than\n",
    "greedy search since it has to compute and store multiple potential sequences.\n",
    "\n",
    "**Note:** beam search with `num_beams=1` is identical to greedy search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnLgt1_mdCat",
    "outputId": "94b4afc4-e839-4338-9b95-5698814055bd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search generated text: \n",
      "['[BOS] \" it is true , \" he said . \" it is true that it is true , but it is true , and it is true that it is true . it is true that it is true , but it is true , and it is true that it is true . it seems to me that it is true , but it is true that it is true , and that it is not to say that it is true , but it is true that it is true , and that it is not to say that it is true , but it is true that it is true that it is true , and that it is true that it is not']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.BeamSampler(num_beams=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Beam search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koPxZYxmdCau"
   },
   "source": [
    "Similar to greedy search, beam search quickly starts repeating itself, since it is still\n",
    "a deterministic method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mD_R_6nGdCau"
   },
   "source": [
    "### Random search\n",
    "\n",
    "Random search is our first probabilistic method. At each time step, it samples the next\n",
    "token using the softmax probabilities provided by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Na-ip2rWdCau",
    "outputId": "06904cbf-4ac9-46ed-dd15-221b6e4bc166",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random search generated text: \n",
      "[\"[BOS] the conversation , although they did not call . the household was soon settled on the quantities out of sorts and vain possessed themselves . the front , elsie had breathlessly distorted of time with her not what she wore , but one day large aside , and her hair had been thick . swift and mole children rode amid quite bright still . after that she had finished its feeds with her head and fell and made over what was the last look of things at the star ' bins placed over the handkerchief and waited stoinfully forward she sometimes in order to keep sitting across it . a cry went by the\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.RandomSampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Random search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeWrzns9dCau"
   },
   "source": [
    "Voilà, no repetitions! However, with random search, we may see some nonsensical words\n",
    "appearing since any word in the vocabulary has a chance of appearing with this sampling\n",
    "method. This is fixed by our next search utility, top-k search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Le1jaivWdCau"
   },
   "source": [
    "### Top-K search\n",
    "\n",
    "Similar to random search, we sample the next token from the probability distribution\n",
    "provided by the model. The only difference is that here, we select out the top `k` most\n",
    "probable tokens, and distribute the probability mass over them before sampling. This way,\n",
    "we won't be sampling from low probability tokens, and hence we would have less\n",
    "nonsensical words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5w4EfD8wdCav",
    "outputId": "3c8f65ee-0c64-46ae-bc2c-ffec6cbe3e2f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-K search generated text: \n",
      "['[BOS] \" that \\' s true , \" he said , \" i am sorry i will tell you that the two - - and you are not in a hurry to see me . you know what the matter would not be done . i have been going to be a prisoner . i have been thinking over here , i have been trying to find out what we should have done . i am sure i will tell you what to say , my brother , and i shall not do the work of the same thing . he does not know how you came back from my father and mother , as the two brothers of the gods and my youngest , you see the']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopKSampler(k=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I8cMPu5dCav"
   },
   "source": [
    "### Top-P search\n",
    "\n",
    "Even with the top-k search, there is something to improve upon. With top-k search, the\n",
    "number `k` is fixed, which means it selects the same number of tokens for any probability\n",
    "distribution. Consider two scenarios, one where the probability mass is concentrated over\n",
    "2 words and another where the probability mass is evenly concentrated across 10. Should\n",
    "we choose `k=2` or `k=10`? There is no one size that fits all `k` here.\n",
    "\n",
    "This is where top-p search comes in! Instead of choosing a `k`, we choose a probability\n",
    "`p` that we want the probabilities of the top tokens to sum up to. This way, we can\n",
    "dynamically adjust the `k` based on the probability distribution. By setting `p=0.9`, if\n",
    "90% of the probability mass is concentrated on the top 2 tokens, we can filter out the\n",
    "top 2 tokens to sample from. If instead the 90% is distributed over 10 tokens, it will\n",
    "similarly filter out the top 10 tokens to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPXLSNwidCav",
    "outputId": "1844402f-b89e-4647-d68d-b43cfcfeb908",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-P search generated text: \n",
      "['[BOS] \" that is the true name of the name of the keel , and that it is not that i know you are my own , and it is very much to you , that , i believe it is very difficult to find it difficult to get out of it , and not that you will be a good thing to do for you to live here , and not often with you in the way of travelling with a great deal of rest , and it is to be found that the two boys have to look for them . i don \\' t believe that the child has a bad heart and makes a little squeak of the mut']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_hub.samplers.TopPSampler(p=0.5)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-P search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MvdUT3bdCav"
   },
   "source": [
    "### Using callbacks for text generation\n",
    "\n",
    "We can also wrap the utilities in a callback, which allows you to print out a prediction\n",
    "sequence for every epoch of the model! Here is an example of a callback for top-k search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ls6wWFWNdCav",
    "outputId": "3a624e31-765b-4a71-f19e-e19048162bf5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 09:40:39.330675: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4327584292250278623\n",
      "2024-11-09 09:40:39.330770: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6965722924101409755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-K search generated text: \n",
      "['[BOS] \" it was not until she came to the palace in the hall , where the sun was shining . she sat looking at her mother \\' s face , with a black hairy blue , which had been tied round a necklace . she had been a good many times and had been combed by the spruce tree . a long and low bush of her tail - - the white - haired - - but a little , with the black shadows on the floor . it looked like a little white bear in her eyes . her head was very thin , and she was a great , white and white beard , and her hair']\n",
      "\n",
      "1/1 - 9s - 9s/step - loss: 3.7595 - perplexity: 42.9551\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 09:40:46.814181: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 4327584292250278623\n",
      "2024-11-09 09:40:46.814265: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 6965722924101409755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-K search generated text: \n",
      "['[BOS] \" i am not going to be allowed to have a little organized house at the same time , and when the preceding evening comes back with you ; but it is so well to be the case , and the other , when it comes to the conclusion that i will not be much more than you will not let the confinement of any kind . in that i am not to say to you , i have been in a prussy , for i cannot say that i cannot explain that it has to me . it may be that i have been more fortunate , in my life , and it may']\n",
      "\n",
      "1/1 - 7s - 7s/step - loss: 3.8710 - perplexity: 48.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f9c927026d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class TopKTextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.sampler = keras_hub.samplers.TopKSampler(k)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        output_tokens = self.sampler(\n",
    "            next=next,\n",
    "            prompt=prompt_tokens,\n",
    "            index=1,\n",
    "        )\n",
    "        txt = tokenizer.detokenize(output_tokens)\n",
    "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
    "\n",
    "\n",
    "text_generation_callback = TopKTextGenerator(k=10)\n",
    "# Dummy training loop to demonstrate callback.\n",
    "model.fit(train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2-Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# To speed up training and generation, we use preprocessor of length 128\n",
    "# instead of full length 1024.\n",
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=128,\n",
    ")\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\", preprocessor=preprocessor\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.prefetch_op._PrefetchDataset"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1731167352.601774  285864 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2327/Unknown \u001b[1m601s\u001b[0m 240ms/step - accuracy: 0.3069 - loss: 3.6951"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1731167931.382240  285864 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2444/Unknown \u001b[1m649s\u001b[0m 248ms/step - accuracy: 0.3071 - loss: 3.6933"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 10:59:38.105590: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m649s\u001b[0m 248ms/step - accuracy: 0.3071 - loss: 3.6933\n",
      "Epoch 2/3\n",
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m593s\u001b[0m 240ms/step - accuracy: 0.3128 - loss: 3.6326\n",
      "Epoch 3/3\n",
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m593s\u001b[0m 240ms/step - accuracy: 0.3129 - loss: 3.6343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f9c903780d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "\n",
    "# Linearly decaying learning rate.\n",
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-5,\n",
    "    decay_steps= 200 * num_epochs,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gpt2_lm.fit(raw_train_ds, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model For Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_base_en/2/download/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:00<00:00, 546kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_base_en/2/download/model.weights.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414M/414M [00:04<00:00, 89.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_base_en/2/download/tokenizer.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 548/548 [00:00<00:00, 1.33MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_base_en/2/download/assets/tokenizer/vocabulary.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208k/208k [00:00<00:00, 7.86MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Will Used a BertMaskedLM\n",
    "\n",
    "keras_hub.models.BertMaskedLM(backbone, preprocessor=None, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1731205344.972978  491620 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2320/Unknown \u001b[1m1181s\u001b[0m 500ms/step - loss: 1.2632 - sparse_categorical_accuracy: 0.3731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1731206515.037613  491615 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2024-11-09 21:42:04.850219: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'input_add_reduce_fusion', 88 bytes spill stores, 88 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2444/Unknown \u001b[1m1254s\u001b[0m 504ms/step - loss: 1.2486 - sparse_categorical_accuracy: 0.3777"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 21:43:06.775721: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-09 21:43:06.775825: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 3076185181018547487\n",
      "/apps/software/standard/core/jupyterlab/3.6.3-py3.11/lib/python3.11/contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1254s\u001b[0m 504ms/step - loss: 1.2483 - sparse_categorical_accuracy: 0.3777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7eff0c524c10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Pretrained language model.\n",
    "masked_lm = keras_hub.models.BertMaskedLM.from_preset(\n",
    "    \"bert_base_en_uncased\",\n",
    ")\n",
    "#masked_lm.fit(x=raw_train_ds)\n",
    "\n",
    "# Re-compile (e.g., with a new learning rate).\n",
    "masked_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(5e-5),\n",
    "    jit_compile=True,\n",
    ")\n",
    "# Access backbone programmatically (e.g., to change `trainable`).\n",
    "masked_lm.backbone.trainable = False\n",
    "# Fit again.\n",
    "masked_lm.fit(x=raw_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
