{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeUxza2N8Xnt"
   },
   "source": [
    "## Part I: Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "Fvnyhv5GTSrp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "1mC8tpK9TlwJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# If there are errors with SSL downloading involving self-signed certificates,\n",
    "# it may be that your Python version was recently installed on the current machine.\n",
    "# See: https://github.com/tensorflow/tensorflow/issues/10779\n",
    "# To fix, run the command: /Applications/Python\\ 3.7/Install\\ Certificates.command\n",
    "#   ...replacing paths as necessary.\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "t-lUuwKN8nmH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "\n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "DV7ItgqZ8nyW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 32, 32, 3) (64,)\n",
      "1 (64, 32, 32, 3) (64,)\n",
      "2 (64, 32, 32, 3) (64,)\n",
      "3 (64, 32, 32, 3) (64,)\n",
      "4 (64, 32, 32, 3) (64,)\n",
      "5 (64, 32, 32, 3) (64,)\n",
      "6 (64, 32, 32, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "zRL6OeSn8n78",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /cpu:0\n"
     ]
    }
   ],
   "source": [
    "# Set up some global variables\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "uPMJoHoZ9H7L",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_np:\n",
      " [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]] \n",
      "\n",
      "x_flat_np:\n",
      " tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      " [12 13 14 15 16 17 18 19 20 21 22 23]], shape=(2, 12), dtype=int32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Helper Functions\n",
    "def flatten(x):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - TensorFlow Tensor of shape (N, D1, ..., DM)\n",
    "\n",
    "    Output:\n",
    "    - TensorFlow Tensor of shape (N, D1 * ... * DM)\n",
    "    \"\"\"\n",
    "    N = tf.shape(x)[0]\n",
    "    return tf.reshape(x, (N, -1))\n",
    "\n",
    "def test_flatten():\n",
    "    # Construct concrete values of the input data x using numpy\n",
    "    x_np = np.arange(24).reshape((2, 3, 4))\n",
    "    print('x_np:\\n', x_np, '\\n')\n",
    "    # Compute a concrete output value.\n",
    "    x_flat_np = flatten(x_np)\n",
    "    print('x_flat_np:\\n', x_flat_np, '\\n')\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba4VIHRD_0GT"
   },
   "source": [
    "### Define a TwoLayer Network\n",
    "\n",
    "We will now implement our first neural network with TensorFlow: a fully-connected ReLU network with two hidden layers and no biases on the CIFAR10 dataset. For now we will use only low-level TensorFlow operators to define the network; later we will see how to use the higher-level abstractions provided by tf.keras to simplify the process.\n",
    "\n",
    "We will define the forward pass of the network in the function TwoLayerFCNeuralNetwork; this will accept TensorFlow Tensors for the inputs and weights of the network, and return a TensorFlow Tensor for the scores.\n",
    "\n",
    "After defining the network architecture in the TwoLayerFCNeuralNetwork function, we will test the implementation by checking the shape of the output.\n",
    "\n",
    "It's important that you read and understand this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "M8QkMU5i9IKW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TwoLayerFCNeuralNetwork(x, params):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network; the architecture is:\n",
    "    fully-connected layer -> ReLU -> fully connected layer.\n",
    "    Note that we only need to define the forward pass here; TensorFlow will take\n",
    "    care of computing the gradients for us.\n",
    "\n",
    "    The input to the network will be a minibatch of data, of shape\n",
    "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
    "    and the output layer will produce scores for C classes.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, d1, ..., dM) giving a minibatch of\n",
    "      input data.\n",
    "    - params: A list [w1, w2] of TensorFlow Tensors giving weights for the\n",
    "      network, where w1 has shape (D, H) and w2 has shape (H, C).\n",
    "\n",
    "    Returns:\n",
    "    - scores: A TensorFlow Tensor of shape (N, C) giving classification scores\n",
    "      for the input data x.\n",
    "    \"\"\"\n",
    "    w1, w2 = params                   # Unpack the parameters\n",
    "    x = flatten(x)                    # Flatten the input; now x has shape (N, D)\n",
    "    h = tf.nn.relu(tf.matmul(x, w1))  # Hidden layer: h has shape (N, H)\n",
    "    scores = tf.matmul(h, w2)         # Compute scores of shape (N, C)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "DnELPAAVACU5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def TwoLayerFCNeuralNetwork_test():\n",
    "    hidden_layer_size = 42\n",
    "\n",
    "    # Scoping our TF operations under a tf.device context manager\n",
    "    # lets us tell TensorFlow where we want these Tensors to be\n",
    "    # multiplied and/or operated on, e.g. on a CPU or a GPU.\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 32, 32, 3))\n",
    "        w1 = tf.zeros((32 * 32 * 3, hidden_layer_size))\n",
    "        w2 = tf.zeros((hidden_layer_size, 10))\n",
    "\n",
    "        # Call our TwoLayerFCNeuralNetwork function for the forward pass of the network.\n",
    "        scores = TwoLayerFCNeuralNetwork(x, [w1, w2])\n",
    "\n",
    "    print(scores.shape)\n",
    "\n",
    "TwoLayerFCNeuralNetwork_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iG28KKSAPWl"
   },
   "source": [
    "## Three-Layer ConvNet\n",
    "Here you will complete the implementation of the function ThreeLayerConvNet which will perform the forward pass of a three-layer convolutional network. The network should have the following architecture:\n",
    "\n",
    "A convolutional layer (with bias) with channel_1 filters, each with shape KW1 x KH1, and zero-padding of two\n",
    "ReLU nonlinearity\n",
    "A convolutional layer (with bias) with channel_2 filters, each with shape KW2 x KH2, and zero-padding of one\n",
    "ReLU nonlinearity\n",
    "Fully-connected layer with bias, producing scores for C classes.\n",
    "\n",
    "HINT: For convolutions: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/conv2d; be careful with padding!\n",
    "\n",
    "HINT: For biases: https://www.tensorflow.org/performance/xla/broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "0T-cxhFzACdw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ThreeLayerConvNet(x, params):\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the architecture described above.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images\n",
    "    - params: A list of TensorFlow Tensors giving the weights and biases for the\n",
    "      network; should contain the following:\n",
    "      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving\n",
    "        weights for the first convolutional layer.\n",
    "      - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the\n",
    "        first convolutional layer.\n",
    "      - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)\n",
    "        giving weights for the second convolutional layer\n",
    "      - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the\n",
    "        second convolutional layer.\n",
    "      - fc_w: TensorFlow Tensor giving weights for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "      - fc_b: TensorFlow Tensor giving biases for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "    \"\"\"\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    scores = None\n",
    "    ############################################################################\n",
    "    # TODO: Implement the forward pass for the three-layer ConvNet.            #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    conv1 = tf.nn.conv2d(x, conv_w1, strides=[1, 1, 1, 1], padding='SAME') + conv_b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    #pool1 = tf.nn.max_pool2d(relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Second convolutional layer: Conv -> ReLU -> Pool\n",
    "    conv2 = tf.nn.conv2d(relu1, conv_w2, strides=[1, 1, 1, 1], padding='SAME') + conv_b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    #pool2 = tf.nn.max_pool2d(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Flatten the output from the second pooling layer\n",
    "    flat = flatten(relu2)\n",
    "    N = tf.shape(flat)[0]  # batch size\n",
    "    flat_shape = tf.shape(flat)[1]  # Compute the size of the flattened vector\n",
    "    \n",
    "    # Reshape the flattened output to match the fully connected layer weight matrix\n",
    "    flat = tf.reshape(flat, (tf.shape(flat)[0], flat_shape))  # Ensure the shape is [batch_size, flat_shape]\n",
    "\n",
    "    # Fully connected layer: Affine transformation (matrix multiplication)\n",
    "    scores = tf.matmul(flat, fc_w) + fc_b\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                              END OF YOUR CODE                            #\n",
    "    ############################################################################\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb5oLOhNAbl9"
   },
   "source": [
    "After defing the forward pass of the three-layer ConvNet above, run the following cell to test your implementation. Like the two-layer network, we run the graph on a batch of zeros just to make sure the function doesn't crash, and produces outputs of the correct shape.\n",
    "\n",
    "When you run this function, scores_np should have shape (64, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "6M7vGsyfACmD",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_np has shape:  (64, 10)\n"
     ]
    }
   ],
   "source": [
    "def ThreeLayerConvNetTest():\n",
    "\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 32, 32, 3))\n",
    "        conv_w1 = tf.zeros((5, 5, 3, 6))\n",
    "        conv_b1 = tf.zeros((6,))\n",
    "        conv_w2 = tf.zeros((3, 3, 6, 9))\n",
    "        conv_b2 = tf.zeros((9,))\n",
    "        fc_w = tf.zeros((32 * 32 * 9, 10))\n",
    "        fc_b = tf.zeros((10,))\n",
    "        params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "        scores = ThreeLayerConvNet(x, params)\n",
    "\n",
    "    # Inputs to convolutional layers are 4-dimensional arrays with shape\n",
    "    # [batch_size, height, width, channels]\n",
    "    print('scores_np has shape: ', scores.shape)\n",
    "\n",
    "ThreeLayerConvNetTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbdYumq5AriO"
   },
   "source": [
    "### Training Step\n",
    "We now define the training_step function performs a single training step. This will take three basic steps:\n",
    "\n",
    "Compute the loss\n",
    "Compute the gradient of the loss with respect to all network weights\n",
    "Make a weight update step using (stochastic) gradient descent.\n",
    "We need to use a few new TensorFlow functions to do all of this:\n",
    "\n",
    "For computing the cross-entropy loss we'll use tf.nn.\n",
    "sparse_softmax_cross_entropy_with_logits: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "\n",
    "For averaging the loss across a minibatch of data we'll use tf.reduce_mean:\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/reduce_mean\n",
    "\n",
    "For computing gradients of the loss with respect to the weights we'll use tf.GradientTape (useful for Eager execution): https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape\n",
    "\n",
    "We'll mutate the weight values stored in a TensorFlow Tensor using tf.compat.v1.assign_sub (\"sub\" is for subtraction): https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/compat/v1/assign_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "Ktt3BqEzAvVx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainingStep(model_fn, x, y, params, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        scores = model_fn(x, params) # Forward pass of the model\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        grad_params = tape.gradient(total_loss, params)\n",
    "\n",
    "        # Make a vanilla gradient descent step on all of the model parameters\n",
    "        # Manually update the weights using assign_sub()\n",
    "        for w, grad_w in zip(params, grad_params):\n",
    "            w.assign_sub(learning_rate * grad_w)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "k4xadUESA02j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainingStep2(model_fn, init_fn, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "\n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model\n",
    "      using TensorFlow; it should have the following signature:\n",
    "      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a\n",
    "      minibatch of image data, params is a list of TensorFlow Tensors holding\n",
    "      the model weights, and scores is a TensorFlow Tensor of shape (N, C)\n",
    "      giving scores for all elements of x.\n",
    "    - init_fn: A Python function that initializes the parameters of the model.\n",
    "      It should have the signature params = init_fn() where params is a list\n",
    "      of TensorFlow Tensors holding the (randomly initialized) weights of the\n",
    "      model.\n",
    "    - learning_rate: Python float giving the learning rate to use for SGD.\n",
    "    \"\"\"\n",
    "    params = init_fn()  # Initialize the model parameters\n",
    "\n",
    "    for t, (x_np, y_np) in enumerate(train_dset):\n",
    "        # Run the graph on a batch of training data.\n",
    "        loss = trainingStep(model_fn, x_np, y_np, params, learning_rate)\n",
    "\n",
    "        # Periodically print the loss and check accuracy on the val set.\n",
    "        if t % print_every == 0:\n",
    "            print('Iteration %d, loss = %.4f' % (t, loss))\n",
    "            check_accuracy(val_dset, x_np, model_fn, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "J2c2oOjqA1Bp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_accuracy(dset, x, model_fn, params):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model, e.g. for validation.\n",
    "\n",
    "    Inputs:\n",
    "    - dset: A Dataset object against which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - model_fn: the Model we will be calling to make predictions on x\n",
    "    - params: parameters for the model_fn to work with\n",
    "\n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for x_batch, y_batch in dset:\n",
    "        scores_np = model_fn(x_batch, params).numpy()\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        num_samples += x_batch.shape[0]\n",
    "        num_correct += (y_pred == y_batch).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8Bi2AQFA_-X"
   },
   "source": [
    "### Initialization\n",
    "We'll use the following utility method to initialize the weight matrices for our models using Kaiming's normalization method.\n",
    "\n",
    "[1] He et al, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification , ICCV 2015, https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "rRYZynwLA5K2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_matrix_with_kaiming_normal(shape):\n",
    "    if len(shape) == 2:\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "    elif len(shape) == 4:\n",
    "        fan_in, fan_out = np.prod(shape[:3]), shape[3]\n",
    "    return tf.keras.backend.random_normal(shape) * np.sqrt(2.0 / fan_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGKP6D2oBLOD"
   },
   "source": [
    "### Train a Two-Layer Network\n",
    "We are finally ready to use all of the pieces defined above to train a two-layer fully-connected network on CIFAR-10.\n",
    "\n",
    "We just need to define a function to initialize the weights of the model, and call train_part2.\n",
    "\n",
    "Defining the weights of the network introduces another important piece of TensorFlow API: tf.Variable. A TensorFlow Variable is a Tensor whose value is stored in the graph and persists across runs of the computational graph; however unlike constants defined with tf.zeros or tf.random_normal, the values of a Variable can be mutated as the graph runs; these mutations will persist across graph runs. Learnable parameters of the network are usually stored in Variables.\n",
    "\n",
    "You don't need to tune any hyperparameters, but you should achieve validation accuracies above 40% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "D8C8_UOfA5Xv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.8848\n",
      "Got 149 / 1000 correct (14.90%)\n",
      "Iteration 100, loss = 2.0034\n",
      "Got 378 / 1000 correct (37.80%)\n",
      "Iteration 200, loss = 1.5548\n",
      "Got 415 / 1000 correct (41.50%)\n",
      "Iteration 300, loss = 1.9043\n",
      "Got 377 / 1000 correct (37.70%)\n",
      "Iteration 400, loss = 1.7129\n",
      "Got 405 / 1000 correct (40.50%)\n",
      "Iteration 500, loss = 1.7573\n",
      "Got 429 / 1000 correct (42.90%)\n",
      "Iteration 600, loss = 1.8939\n",
      "Got 427 / 1000 correct (42.70%)\n",
      "Iteration 700, loss = 1.9229\n",
      "Got 447 / 1000 correct (44.70%)\n"
     ]
    }
   ],
   "source": [
    "def TwoLayerFCNeuralNetwork_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a two-layer network, for use with the\n",
    "    TwoLayerFCNeuralNetwork function defined above.\n",
    "    You can use the `create_matrix_with_kaiming_normal` helper!\n",
    "\n",
    "    Inputs: None\n",
    "\n",
    "    Returns: A list of:\n",
    "    - w1: TensorFlow tf.Variable giving the weights for the first layer\n",
    "    - w2: TensorFlow tf.Variable giving the weights for the second layer\n",
    "    \"\"\"\n",
    "    hidden_layer_size = 4000\n",
    "    w1 = tf.Variable(create_matrix_with_kaiming_normal((3 * 32 * 32, 4000)))\n",
    "    w2 = tf.Variable(create_matrix_with_kaiming_normal((4000, 10)))\n",
    "    return [w1, w2]\n",
    "\n",
    "learning_rate = 1e-2\n",
    "trainingStep2(TwoLayerFCNeuralNetwork, TwoLayerFCNeuralNetwork_init, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsxyBeOBBYuL"
   },
   "source": [
    "### Train a three-layer ConvNet\n",
    "We will now use TensorFlow to train a three-layer ConvNet on CIFAR-10.\n",
    "\n",
    "You need to implement the ThreeLayerConvNet_init function. Recall that the architecture of the network is:\n",
    "\n",
    "Convolutional layer (with bias) with 32 5x5 filters, with zero-padding 2\n",
    "ReLU\n",
    "Convolutional layer (with bias) with 16 3x3 filters, with zero-padding 1\n",
    "ReLU\n",
    "Fully-connected layer (with bias) to compute scores for 10 classes\n",
    "You don't need to do any hyperparameter tuning, but you should see validation accuracies above 43% after one epoch of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "3s4pC1YrA5fB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.9589\n",
      "Got 133 / 1000 correct (13.30%)\n",
      "Iteration 100, loss = 1.7472\n",
      "Got 362 / 1000 correct (36.20%)\n",
      "Iteration 200, loss = 1.5707\n",
      "Got 393 / 1000 correct (39.30%)\n",
      "Iteration 300, loss = 1.6880\n",
      "Got 403 / 1000 correct (40.30%)\n",
      "Iteration 400, loss = 1.7265\n",
      "Got 443 / 1000 correct (44.30%)\n",
      "Iteration 500, loss = 1.6660\n",
      "Got 447 / 1000 correct (44.70%)\n",
      "Iteration 600, loss = 1.6297\n",
      "Got 467 / 1000 correct (46.70%)\n",
      "Iteration 700, loss = 1.6675\n",
      "Got 474 / 1000 correct (47.40%)\n"
     ]
    }
   ],
   "source": [
    "def ThreeLayerConvNet_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a Three-Layer ConvNet, for use with the\n",
    "    ThreeLayerConvNet function defined above.\n",
    "    You can use the `create_matrix_with_kaiming_normal` helper!\n",
    "\n",
    "    Inputs: None\n",
    "\n",
    "    Returns a list containing:\n",
    "    - conv_w1: TensorFlow tf.Variable giving weights for the first conv layer\n",
    "    - conv_b1: TensorFlow tf.Variable giving biases for the first conv layer\n",
    "    - conv_w2: TensorFlow tf.Variable giving weights for the second conv layer\n",
    "    - conv_b2: TensorFlow tf.Variable giving biases for the second conv layer\n",
    "    - fc_w: TensorFlow tf.Variable giving weights for the fully-connected layer\n",
    "    - fc_b: TensorFlow tf.Variable giving biases for the fully-connected layer\n",
    "    \"\"\"\n",
    "    params = None\n",
    "    ############################################################################\n",
    "    # TODO: Initialize the parameters of the three-layer network.              #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    conv_w1 = tf.Variable(create_matrix_with_kaiming_normal([5, 5, 3, 32]))  \n",
    "    conv_b1 = tf.Variable(tf.zeros(32))  \n",
    "    \n",
    "    # Second convolutional layer: (KH2, KW2, channel_1, channel_2)\n",
    "    conv_w2 = tf.Variable(create_matrix_with_kaiming_normal([3, 3, 32, 16]))  \n",
    "    conv_b2 = tf.Variable(tf.zeros(16))  \n",
    "    \n",
    "    # Fully connected layer: Size needs to match the output from the final pooling layer\n",
    "    fc_w = tf.Variable(create_matrix_with_kaiming_normal([32 * 32 * 16, 10])) \n",
    "    fc_b = tf.Variable(tf.zeros(10)) \n",
    "    params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "    return params\n",
    "\n",
    "learning_rate = 3e-3\n",
    "trainingStep2(ThreeLayerConvNet, ThreeLayerConvNet_init, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3w8LGTcBs9y"
   },
   "source": [
    "## Part III: Keras Model Subclassing API\n",
    "Implementing a neural network using the low-level TensorFlow API is a good way to understand how TensorFlow works, but it's a little inconvenient - we had to manually keep track of all Tensors holding learnable parameters. This was fine for a small network, but could quickly become unweildy for a large complex model.\n",
    "\n",
    "Fortunately TensorFlow 2.0 provides higher-level APIs such as tf.keras which make it easy to build models out of modular, object-oriented layers. Further, TensorFlow 2.0 uses eager execution that evaluates operations immediately, without explicitly constructing any computational graphs. This makes it easy to write and debug models, and reduces the boilerplate code.\n",
    "\n",
    "In this part of the notebook we will define neural network models using the tf.keras.Model API. To implement your own model, you need to do the following:\n",
    "\n",
    "Define a new class which subclasses tf.keras.Model. Give your class an intuitive name that describes it, like twoLayerFC or threeLayerConvNet.\n",
    "In the initializer __init__() for your new class, define all the layers you need as class attributes. The tf.keras.layers package provides many common neural-network layers, like tf.keras.layers.Dense for fully-connected layers and tf.keras.layers.Conv2D for convolutional layers. Under the hood, these layers will construct Variable Tensors for any learnable parameters. Warning: Don't forget to call super(YourModelName, self).__init__() as the first line in your initializer!\n",
    "Implement the call() method for your class; this implements the forward pass of your model, and defines the connectivity of your network. Layers defined in __init__() implement __call__() so they can be used as function objects that transform input Tensors into output Tensors. Don't define any new layers in call(); any layers you want to use in the forward pass should be defined in __init__().\n",
    "After you define your tf.keras.Model subclass, you can instantiate it and use it like the model functions from Part II.\n",
    "\n",
    "Keras Model Subclassing API: Two-Layer Network\n",
    "Here is a concrete example of using the tf.keras.Model API to define a two-layer network. There are a few new bits of API to be aware of here:\n",
    "\n",
    "We use an Initializer object to set up the initial values of the learnable parameters of the layers; in particular tf.initializers.VarianceScaling gives behavior similar to the Kaiming initialization method we used in Part II. You can read more about it here: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/initializers/VarianceScaling\n",
    "\n",
    "We construct tf.keras.layers.Dense objects to represent the two fully-connected layers of the model. In addition to multiplying their input by a weight matrix and adding a bias vector, these layer can also apply a nonlinearity for you. For the first layer we specify a ReLU activation function by passing activation='relu' to the constructor; the second layer uses softmax activation function. Finally, we use tf.keras.layers.Flatten to flatten the output from the previous fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "9gAEujFYBh4k",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "class twoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(twoLayerFC, self).__init__()\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def twoLayerFC_test():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = twoLayerFC(hidden_size, num_classes)\n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "\n",
    "twoLayerFC_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KU3U09CB99C"
   },
   "source": [
    "### Keras Model Subclassing API: Three-Layer ConvNet\n",
    "Now it's your turn to implement a three-layer ConvNet using the tf.keras.Model API. Your model should have the same architecture used in Part II:\n",
    "\n",
    "Convolutional layer with 5 x 5 kernels, with zero-padding of 2\n",
    "ReLU nonlinearity\n",
    "Convolutional layer with 3 x 3 kernels, with zero-padding of 1\n",
    "ReLU nonlinearity\n",
    "Fully-connected layer to give class scores\n",
    "Softmax nonlinearity\n",
    "You should initialize the weights of your network using the same initialization method as was used in the two-layer network above.\n",
    "\n",
    "Hint: Refer to the documentation for tf.keras.layers.Conv2D and tf.keras.layers.Dense:\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "enSGhF2GCDVx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class threeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super(threeLayerConvNet, self).__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "        self.conv1 = tf.keras.layers.Conv2D(channel_1, (5, 5), padding='same', activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(channel_2, (3, 3), padding='same', activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        scores = self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYuoOEzxDHN_"
   },
   "source": [
    "Once you complete the implementation of the threeLayerConvNet above you can run the following to ensure that your implementation does not crash and produces outputs of the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "LLmOo_bPDHbJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def threeLayerConvNet_test():\n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = threeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 32, 32, 3))\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "\n",
    "threeLayerConvNet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bHuYTJQDp5B"
   },
   "source": [
    "### Keras Model Subclassing API: Eager Training\n",
    "While keras models have a builtin training loop (using the model.fit), sometimes you need more customization. Here's an example, of a training loop implemented with eager execution.\n",
    "\n",
    "In particular, notice tf.GradientTape. Automatic differentiation is used in the backend for implementing backpropagation in frameworks like TensorFlow. During eager execution, tf.GradientTape is used to trace operations for computing gradients later. A particular tf.GradientTape can only compute one gradient; subsequent calls to tape will throw a runtime error.\n",
    "\n",
    "TensorFlow 2.0 ships with easy-to-use built-in metrics under tf.keras.metrics module. Each metric is an object, and we can use update_state() to add observations and reset_state() to clear all observations. We can get the current result of a metric by calling result() on the metric object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "VrVCykgiDoer",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_part3(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "\n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "\n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"\n",
    "    with tf.device(device):\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "\n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_state()\n",
    "            train_accuracy.reset_state()\n",
    "\n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape:\n",
    "\n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "\n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "\n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_state()\n",
    "                        val_accuracy.reset_state()\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "\n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {:.4f}, Accuracy: {:.4f}, Val Loss: {:.4f}, Val Accuracy: {:.4f}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result(),\n",
    "                                             train_accuracy.result()*100,\n",
    "                                             val_loss.result(),\n",
    "                                             val_accuracy.result()*100))\n",
    "                    t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHeDvt8lDx1P"
   },
   "source": [
    "### Keras Model Subclassing API: Train a Two-Layer Network\n",
    "We can now use the tools defined above to train a two-layer network on CIFAR-10. We define the model_init_fn and optimizer_init_fn that construct the model and optimizer respectively when called. Here we want to train the model using stochastic gradient descent with no momentum, so we construct a tf.keras.optimizers.SGD function; you can read about it here.\n",
    "\n",
    "You don't need to tune any hyperparameters here, but you should achieve validation accuracies above 40% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "GXwnvdb-DzoD",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.2726, Accuracy: 6.2500, Val Loss: 2.8284, Val Accuracy: 12.8000\n",
      "Iteration 100, Epoch 1, Loss: 2.2496, Accuracy: 28.4035, Val Loss: 1.8974, Val Accuracy: 37.3000\n",
      "Iteration 200, Epoch 1, Loss: 2.0893, Accuracy: 32.1828, Val Loss: 1.8206, Val Accuracy: 39.8000\n",
      "Iteration 300, Epoch 1, Loss: 2.0108, Accuracy: 33.9909, Val Loss: 1.8763, Val Accuracy: 38.5000\n",
      "Iteration 400, Epoch 1, Loss: 1.9413, Accuracy: 35.7894, Val Loss: 1.7407, Val Accuracy: 41.2000\n",
      "Iteration 500, Epoch 1, Loss: 1.8973, Accuracy: 36.9137, Val Loss: 1.6783, Val Accuracy: 43.4000\n",
      "Iteration 600, Epoch 1, Loss: 1.8669, Accuracy: 37.7444, Val Loss: 1.7031, Val Accuracy: 43.1000\n",
      "Iteration 700, Epoch 1, Loss: 1.8401, Accuracy: 38.4272, Val Loss: 1.6418, Val Accuracy: 44.8000\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return twoLayerFC(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "training_part3(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bnzam9-eD5hO"
   },
   "source": [
    "### Keras Model Subclassing API: Train a Three-Layer ConvNet\n",
    "Here you should use the tools we've defined above to train a three-layer ConvNet on CIFAR-10. Your ConvNet should use 32 filters in the first convolutional layer and 16 filters in the second layer.\n",
    "\n",
    "To train the model you should use gradient descent with Nesterov momentum 0.9.\n",
    "\n",
    "HINT: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD\n",
    "\n",
    "You don't need to perform any hyperparameter tuning, but you should achieve validation accuracies above 50% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "3r0Rof6uD67F",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 12.0450, Accuracy: 4.6875, Val Loss: 11.8503, Val Accuracy: 10.5000\n",
      "Iteration 100, Epoch 1, Loss: 3.3134, Accuracy: 10.5972, Val Loss: 2.3026, Val Accuracy: 10.1000\n",
      "Iteration 200, Epoch 1, Loss: 2.8105, Accuracy: 10.5955, Val Loss: 2.3026, Val Accuracy: 10.1000\n",
      "Iteration 300, Epoch 1, Loss: 2.6418, Accuracy: 10.4288, Val Loss: 2.3026, Val Accuracy: 10.1000\n",
      "Iteration 400, Epoch 1, Loss: 2.5572, Accuracy: 10.4465, Val Loss: 2.3026, Val Accuracy: 10.1000\n",
      "Iteration 500, Epoch 1, Loss: 2.5064, Accuracy: 10.2888, Val Loss: 2.3026, Val Accuracy: 10.1000\n",
      "Iteration 600, Epoch 1, Loss: 2.4725, Accuracy: 10.2277, Val Loss: 2.3026, Val Accuracy: 10.1000\n",
      "Iteration 700, Epoch 1, Loss: 2.4482, Accuracy: 10.1574, Val Loss: 2.3026, Val Accuracy: 10.1000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    model = threeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    learning_rate = 3e-3  # As specified\n",
    "    momentum = 0.9  # Nesterov momentum\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=True)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "training_part3(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsgyZELNEBTf"
   },
   "source": [
    "## Part IV: Keras Sequential API\n",
    "In Part III we introduced the tf.keras.Model API, which allows you to define models with any number of learnable layers and with arbitrary connectivity between layers.\n",
    "\n",
    "However for many models you don't need such flexibility - a lot of models can be expressed as a sequential stack of layers, with the output of each layer fed to the next layer as input. If your model fits this pattern, then there is an even easier way to define your model: using tf.keras.Sequential. You don't need to write any custom classes; you simply call the tf.keras.Sequential constructor with a list containing a sequence of layer objects.\n",
    "\n",
    "One complication with tf.keras.Sequential is that you must define the shape of the input to the model by passing a value to the input_shape of the first layer in your model.\n",
    "\n",
    "Keras Sequential API: Two-Layer Network\n",
    "In this subsection, we will rewrite the two-layer fully-connected network using tf.keras.Sequential, and train it using the training loop defined above.\n",
    "\n",
    "You don't need to perform any hyperparameter tuning here, but you should see validation accuracies above 40% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "Lek8QCNEEDGA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.1494, Accuracy: 7.8125, Val Loss: 2.9321, Val Accuracy: 12.4000\n",
      "Iteration 100, Epoch 1, Loss: 2.2213, Accuracy: 28.4189, Val Loss: 1.9004, Val Accuracy: 37.5000\n",
      "Iteration 200, Epoch 1, Loss: 2.0733, Accuracy: 32.2761, Val Loss: 1.8821, Val Accuracy: 38.0000\n",
      "Iteration 300, Epoch 1, Loss: 1.9968, Accuracy: 34.2452, Val Loss: 1.8683, Val Accuracy: 37.3000\n",
      "Iteration 400, Epoch 1, Loss: 1.9276, Accuracy: 36.1245, Val Loss: 1.7215, Val Accuracy: 42.3000\n",
      "Iteration 500, Epoch 1, Loss: 1.8857, Accuracy: 37.1039, Val Loss: 1.6550, Val Accuracy: 43.3000\n",
      "Iteration 600, Epoch 1, Loss: 1.8569, Accuracy: 37.9498, Val Loss: 1.6994, Val Accuracy: 41.8000\n",
      "Iteration 700, Epoch 1, Loss: 1.8302, Accuracy: 38.6145, Val Loss: 1.6378, Val Accuracy: 43.6000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    input_shape = (32, 32, 3)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',\n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "training_part3(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Vbvvp3HEJlg"
   },
   "source": [
    "### Abstracting Away the Training Loop\n",
    "In the previous examples, we used a customised training loop to train models (e.g. train_part34). Writing your own training loop is only required if you need more flexibility and control during training your model. Alternately, you can also use built-in APIs like tf.keras.Model.fit() and tf.keras.Model.evaluate to train and evaluate a model. Also remember to configure your model for training by calling `tf.keras.Model.compile.\n",
    "\n",
    "You don't need to perform any hyperparameter tuning here, but you should see validation and test accuracies above 42% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "TybJsElPEUrU",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m766/766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - loss: 2.0068 - sparse_categorical_accuracy: 0.3404 - val_loss: 1.6609 - val_sparse_categorical_accuracy: 0.4130\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.6507 - sparse_categorical_accuracy: 0.4322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6566872596740723, 0.42750000953674316]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUmTp0-bEZFo"
   },
   "source": [
    "### Keras Sequential API: Three-Layer ConvNet\n",
    "Here you should use tf.keras.Sequential to reimplement the same three-layer ConvNet architecture used in Part II and Part III. As a reminder, your model should have the following architecture:\n",
    "\n",
    "Convolutional layer with 32 5x5 kernels, using zero padding of 2\n",
    "ReLU nonlinearity\n",
    "Convolutional layer with 16 3x3 kernels, using zero padding of 1\n",
    "ReLU nonlinearity\n",
    "Fully-connected layer giving class scores\n",
    "Softmax nonlinearity\n",
    "You should initialize the weights of the model using a tf.initializers.VarianceScaling as above.\n",
    "\n",
    "You should train the model using Nesterov momentum 0.9.\n",
    "\n",
    "You don't need to perform any hyperparameter search, but you should achieve accuracy above 45% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "UK7b2VHREahx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.3888, Accuracy: 14.0625, Val Loss: 2.3621, Val Accuracy: 11.1000\n",
      "Iteration 100, Epoch 1, Loss: 2.0370, Accuracy: 28.0786, Val Loss: 1.8745, Val Accuracy: 35.2000\n",
      "Iteration 200, Epoch 1, Loss: 1.9268, Accuracy: 32.2683, Val Loss: 1.7288, Val Accuracy: 40.7000\n",
      "Iteration 300, Epoch 1, Loss: 1.8600, Accuracy: 34.7280, Val Loss: 1.6847, Val Accuracy: 42.4000\n",
      "Iteration 400, Epoch 1, Loss: 1.7965, Accuracy: 36.9623, Val Loss: 1.6233, Val Accuracy: 44.6000\n",
      "Iteration 500, Epoch 1, Loss: 1.7533, Accuracy: 38.3078, Val Loss: 1.5804, Val Accuracy: 46.6000\n",
      "Iteration 600, Epoch 1, Loss: 1.7249, Accuracy: 39.4239, Val Loss: 1.5434, Val Accuracy: 47.0000\n",
      "Iteration 700, Epoch 1, Loss: 1.6986, Accuracy: 40.4289, Val Loss: 1.5085, Val Accuracy: 47.9000\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        # First convolutional layer: 32 filters, 5x5 kernel, padding=2, ReLU\n",
    "        tf.keras.layers.Conv2D(32, (5, 5), padding='same', activation='relu', \n",
    "                               kernel_initializer=tf.initializers.VarianceScaling()),\n",
    "\n",
    "        # Second convolutional layer: 16 filters, 3x3 kernel, padding=1, ReLU\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', \n",
    "                               kernel_initializer=tf.initializers.VarianceScaling()),\n",
    "\n",
    "        # Flatten before the fully connected layer\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        # Fully connected layer: output 10 classes (CIFAR-10)\n",
    "        tf.keras.layers.Dense(10, activation='softmax',\n",
    "                              kernel_initializer=tf.initializers.VarianceScaling())\n",
    "    ])\n",
    "\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4, momentum=0.9, nesterov=True)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "training_part3(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1UtU9vFEdzF"
   },
   "source": [
    "We will also train this model with the built-in training loop APIs provided by TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "OpK0E6MtEhz-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m766/766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 1.7753 - sparse_categorical_accuracy: 0.3707 - val_loss: 1.3596 - val_sparse_categorical_accuracy: 0.5240\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3568 - sparse_categorical_accuracy: 0.5251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3711076974868774, 0.5151000022888184]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5EgkF87E_ci"
   },
   "source": [
    "### Part V: BONUS POINT - CIFAR-10 open-ended challenge\n",
    "In this section you can experiment with whatever ConvNet architecture you'd like on CIFAR-10.\n",
    "\n",
    "You should experiment with architectures, hyperparameters, loss functions, regularization, or anything else you can think of to train a model that achieves at least 70% accuracy on the validation set within 10 epochs. You can use the built-in train function, the training_part3 function from above, or implement your own training loop.\n",
    "\n",
    "Describe what you did at the end of the notebook.\n",
    "\n",
    "Some things you can try:\n",
    "Filter size: Above we used 5x5 and 3x3; is this optimal?\n",
    "Number of filters: Above we used 16 and 32 filters. Would more or fewer do better?\n",
    "Pooling: We didn't use any pooling above. Would this improve the model?\n",
    "Normalization: Would your model be improved with batch normalization, layer normalization, group normalization, or some other normalization strategy?\n",
    "Network architecture: The ConvNet above has only three layers of trainable parameters. Would a deeper model do better?\n",
    "Global average pooling: Instead of flattening after the final convolutional layer, would global average pooling do better? This strategy is used for example in Google's Inception network and in Residual Networks.\n",
    "Regularization: Would some kind of regularization improve performance? Maybe weight decay or dropout?\n",
    "NOTE: Batch Normalization / Dropout\n",
    "If you are using Batch Normalization and Dropout, remember to pass is_training=True if you use the training_part3() function. BatchNorm and Dropout layers have different behaviors at training and inference time. training is a specific keyword argument reserved for this purpose in any tf.keras.Model's call() function. Read more about this here : https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods\n",
    "\n",
    "Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and other hyperparameters. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are not required to implement any of these, but don't miss the fun if you have time!\n",
    "\n",
    "Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.\n",
    "Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "Model ensembles\n",
    "Data augmentation\n",
    "New Architectures\n",
    "ResNets where the input from the previous layer is added to the output.\n",
    "DenseNets where inputs into previous layers are concatenated together.\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IB7m9B_LJ7OX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
